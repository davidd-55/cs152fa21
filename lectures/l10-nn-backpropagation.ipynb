{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1c7670f",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8744658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82b0723",
   "metadata": {},
   "source": [
    "## Create fake input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef6e76d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of training examples\n",
    "N = 100\n",
    "\n",
    "# Number of inputs and outputs (based on diagram)\n",
    "nx = 3\n",
    "ny = 2\n",
    "\n",
    "# Random inputs and outputs (just for sake of computation)\n",
    "X = torch.randn(N, nx)\n",
    "Y = torch.randn(N, ny)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42456bc9",
   "metadata": {},
   "source": [
    "## Create a simple model based on the diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa0fa686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(A, W, b):\n",
    "    return A @ W.T + b\n",
    "\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + torch.exp(-Z))\n",
    "\n",
    "\n",
    "# A two-layer network with 3 neurons in the only hidden layer\n",
    "n0 = nx\n",
    "n1 = 3\n",
    "n2 = ny\n",
    "\n",
    "# Layer 1 parameters\n",
    "W1 = torch.randn(n1, n0)\n",
    "b1 = torch.randn(n1)\n",
    "\n",
    "# Layer 2 parameters\n",
    "W2 = torch.randn(n2, n1)\n",
    "b2 = torch.randn(n2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd106642",
   "metadata": {},
   "source": [
    "## Compute model output (forward propagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44f95c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "A0 = X\n",
    "\n",
    "# Forward propagation\n",
    "Z1 = linear(A0, W1, b1)\n",
    "A1 = sigmoid(Z1)\n",
    "\n",
    "Z2 = linear(A1, W2, b2)\n",
    "A2 = sigmoid(Z2)\n",
    "\n",
    "Yhat = A2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc7d473",
   "metadata": {},
   "source": [
    "## Backpropagation from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a03c6911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -1.1888227462768555\n"
     ]
    }
   ],
   "source": [
    "# Compute loss as the mean-square-error\n",
    "bce_loss = torch.mean(Y * torch.log(Yhat) + (1 - Y) * torch.log(1 - Yhat))\n",
    "print(\"Loss:\", bce_loss.item())\n",
    "\n",
    "# Compute gradients for W^[2] and b^[2]\n",
    "# dL_dY = Yhat - Y\n",
    "dL_dY = (Y / Yhat - (1 - Y) / (1 - Yhat)) / 2\n",
    "dY_dZ2 = Yhat * (1 - Yhat)\n",
    "\n",
    "dZ2 = dL_dY * dY_dZ2\n",
    "\n",
    "dW2 = (1 / N) * dZ2.T @ A1\n",
    "db2 = dZ2.mean(dim=0)\n",
    "\n",
    "# Compute gradients for W^[1] and b^[1]\n",
    "dZ1 = dZ2 @ W2 * ((A1 * (1 - A1)))\n",
    "\n",
    "dW1 = (1 / N) * dZ1.T @ X\n",
    "db1 = dZ1.mean(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc82dbc",
   "metadata": {},
   "source": [
    "## Forward and backward propagation using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48303e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -1.1888227462768555\n"
     ]
    }
   ],
   "source": [
    "# Let's copy the Ws and bs from above, but set them\n",
    "# up for auto-differentiation\n",
    "\n",
    "# Layer 1 parameters\n",
    "W1Auto = W1.clone().detach().requires_grad_(True)\n",
    "b1Auto = b1.clone().detach().requires_grad_(True)\n",
    "\n",
    "# Layer 2 parameters\n",
    "W2Auto = W2.clone().detach().requires_grad_(True)\n",
    "b2Auto = b2.clone().detach().requires_grad_(True)\n",
    "\n",
    "# Forward propagation (same as above, but using PyTorch functionality)\n",
    "A0 = X\n",
    "Z1 = torch.nn.functional.linear(A0, W1Auto, b1Auto)\n",
    "A1 = torch.sigmoid(Z1)\n",
    "\n",
    "Z2 = torch.nn.functional.linear(A1, W2Auto, b2Auto)\n",
    "A2 = torch.sigmoid(Z2)\n",
    "Yhat = A2\n",
    "\n",
    "# Compute loss (same as above)\n",
    "# bce_loss = torch.mean(Y * torch.log(Yhat) + (1 - Y) * torch.log(1 - Yhat))\n",
    "bce_loss = -torch.nn.functional.binary_cross_entropy(Yhat, Y)\n",
    "print(\"Loss:\", bce_loss.item())\n",
    "\n",
    "# Automatically compute derivatives\n",
    "bce_loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf2d38c",
   "metadata": {},
   "source": [
    "## Compare computed gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ae248e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We shouldn't compare floating-point numbers using \"==\" since results\n",
    "#  can differ based on the order of operations.\n",
    "assert torch.allclose(dW2, W2Auto.grad)\n",
    "assert torch.allclose(db2, b2Auto.grad)\n",
    "\n",
    "assert torch.allclose(dW1, W1Auto.grad)\n",
    "assert torch.allclose(db1, b1Auto.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06397d0e",
   "metadata": {},
   "source": [
    "- Adding additional layers\n",
    "- Changing the loss function\n",
    "- Changing the activation function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fc61a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
